<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bootstrap on Guangchuang Yu</title>
    <link>http://guangchuangyu.github.io/tags/bootstrap/</link>
    <description>Recent content in Bootstrap on Guangchuang Yu</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 Jun 2011 06:10:37 +0800</lastBuildDate>
    <atom:link href="http://guangchuangyu.github.io/tags/bootstrap/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Five things biologists should know about statistics</title>
      <link>http://guangchuangyu.github.io/2011/06/five-things-biologists-should-know-about-statistics/</link>
      <pubDate>Fri, 24 Jun 2011 06:10:37 +0800</pubDate>
      
      <guid>http://guangchuangyu.github.io/2011/06/five-things-biologists-should-know-about-statistics/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.ebi.ac.uk/~birney/&#34;&gt;Ewan Birney&lt;/a&gt;最近的一篇博文（&lt;a href=&#34;http://genomeinformatician.blogspot.com/2011/06/five-statistical-things-i-wished-i-had.html&#34;&gt;Five statistical things I wished I had been taught 20 years ago&lt;/a&gt;
）讲述了统计对于生物学的重要性。&lt;/p&gt;

&lt;p&gt;一开始从RA
Fisher讲起，说生物压根就是统计。Fisher是个农业学家，他所建立的那些统计方法，都是从生物学问题出发。&lt;/p&gt;

&lt;p&gt;Ewan所谈及的五个方面分别是：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;1. Non parametric statistics. These are statistical tests which make a
bare minimum of assumptions of underlying distributions; in biology we
are rarely confident that we know the underlying distribution, and hand
waving about central limit theorem can only get you so far. Wherever
possible you should use a non parameteric test. This is Mann-Whitney (or
Wilcoxon if you prefer) for testing &amp;ldquo;medians&amp;rdquo; (Medians is in quotes
because this is not quite true. They test something which is closely
related to the median) of two distributions, Spearman&amp;rsquo;s Rho (rather
pearson&amp;rsquo;s r2) for correlation, and the Kruskal test rather than ANOVAs
(though if I get this right, you can&amp;rsquo;t in Kruskal do the more
sophisticated nested models you can do with ANOVA). Finally, don&amp;rsquo;t
forget the rather wonderful Kolmogorov-Smirnov (I always think it sounds
like really good vodka) test of whether two sets of observations come
from the same distribution. All of these methods have a basic theme of
doing things on the rank of items in a distribution, not the actual
level. So - if in doubt, do things on the rank of metric, rather than
the metric itself.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;学校里教统计，多半是t检验和ANOVA，这些方法都有assumption需要满足，比如正态分布啥的。多半大家是默认它满足，然后就开始套着用，这是比较危险的，如果assumption不满足，或者数据中有outliers，都可能会导致错误的结论。&lt;/p&gt;

&lt;p&gt;今年&lt;a href=&#34;http://guangchuangyu.github.io/2010/11/phosphoproteome-profile-of-human-lung-cancer-cell-line-a549/&#34;&gt;磷酸化谱的文章&lt;/a&gt;，用到了免疫组化，实验结果的量化是由医生给出来打分值，癌组织和癌旁组织两组数据，免疫组化的数据不可能用参数统计，这个结果我就是用Wilcoxon
signed rank test去做检验。文中所提出的其它非参统计方法，全都不会。囧&lt;/p&gt;

&lt;p&gt;关于相关性，这里有篇文章，比较了Pearson和Spearman： &lt;a href=&#34;http://www.jstor.org/stable/2346598&#34;&gt;On the Effects of
Non-Normality on the Distribution of the Sample Product-Moment
Correlation Coefficient&lt;/a&gt; (Kowalski,
1975)&lt;/p&gt;

&lt;p&gt;还有文章说用Kendall&amp;rsquo;s tau比Spearman&amp;rsquo;s Rho要好： Newson R. &lt;a href=&#34;http://www.stata-journal.com/sjpdf.html?articlenum=st0007&#34;&gt;Parameters
behind &amp;ldquo;nonparametric&amp;rdquo; statistics: Kendall&amp;rsquo;s tau,Somers&amp;rsquo; D and median
differences&lt;/a&gt;.
Stata Journal 2002; 2(1):45-64.&lt;/p&gt;

&lt;p&gt;虽然作者强调非参统计，但是如果数据分布满足参数统计的assumption的话，还是用参数统计好，更加powerful。这个可能需要我们在做统计之前对数据分布做一下检验。不过正态检验其实用处也不大，小样本的话，不够powerful，大样本的话，即使不是正态分布，t-test和ANOVA也是很robust的。&lt;/p&gt;

&lt;p&gt;如果不满足参数统计检验的话，也不一定就得用非参统计，不负责任地说，可能用&lt;a href=&#34;http://guangchuangyu.github.io/2009/07/bootstrap-method/&#34;&gt;bootstrap&lt;/a&gt;效果还更好。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2. R (or I guess S). R is a cranky, odd statistical language/system with
a great scientific plotting package. Its a package written mainly by
statisticians for statisticians, and is rather unforgiving the first
time you use it. It is defnitely worth persevering. It&amp;rsquo;s basically a
combination of excel spreadsheets on steriods (with no data entry. an
Rdata frame is really the same logical set as a excel workbook - able to
handle millions of points, not 1,000s), a statistical methods compendium
(it&amp;rsquo;s usually the case that statistical methods are written first in R,
and you can almost guarantee that there are no bugs in the major
functions - unlike many other scenarios) and a graphical data
exploration tool (in particular lattice and ggplot packages). The syntax
is inconsistent, the documentation sometimes wonderful, often awful and
the learning curve is like the face of the Eiger. But once you&amp;rsquo;ve met
p.adjust(), xyplot() and apply(), you can never turn back.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&#34;http://www.r-project.org&#34;&gt;R&lt;/a&gt;实在是太好用了，习惯用矢量运算之后，我就很少用perl了。不过学生物的，我所见过的人，能用好excel的人不多（我也不会用-,-），会用SPSS的人非常少，SAS从没见过有人用。每次我告诉身边的人，我用的是R，几乎都没人听说过的。在国内，目前主要也就高校里有人用。但至少做生信的，是需要学R的，&lt;a href=&#34;http://www.bioconductor.org/&#34;&gt;Bioconductor&lt;/a&gt;上面那一大堆的软件包，已然是无法回避。&lt;/p&gt;

&lt;p&gt;学生物的人都喜欢有图形界面的软件，像spss这种，点菜单无非是为了选参数，而R这种，变成敲键盘而已，一样的。用编程语言比用分析软件要好，可自动化，而且有利于交流（看一下代码，就知道都干了些什么），像SPSS这种把很多分析模块化，点点鼠标就能把回归模型算出来，固然是好，但是现代的数据分析，已经很少有问题是点个鼠标就能解决的了。&lt;/p&gt;

&lt;p&gt;至于画图，文中提到lattice和ggplot，lattice应该是目前R上面最复杂的图形包，功能比ggplot要强得多，画图速度也比ggplot要快，不过我没用过。只学了ggplot，因为ggplot的语法更加human
friendly，我觉得，学了ggplot后，都会爱上画图的 =,=&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;3. The problem of multiple testing, and how to handle it, either with
the Expected value, or FDR, and the backstop of many of piece of
bioinformatics - large scale permutation. Large scale permutation is
sometimes frowned upon by more maths/distribution purists but often is
the only way to get a sensible sense of whether something is likely &amp;ldquo;by
chance&amp;rdquo; (whatever the latter phrase means - it&amp;rsquo;s a very open question)
given the complex, hetreogenous data we have. 10 years ago perhaps the
lack of large scale compute resources meant this option was less open to
people, but these days basically everyone should be working out how to
appropriate permute the data to allow a good estimate of
&amp;ldquo;surprisingness&amp;rdquo; of an observation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;高通量的组学数据，变得越来越常见，pvalue算的是犯一类错误的概率，组学数据观测点多，而重复少，noise很多，如果单纯卡个pvalue，越高通量的数据，犯二类错误的概率会更大，假阳性没有得到控制。这个越来越重要，这周去给学生上课，我还专门讲了Bonferroni
Method、Benjamini-Hochberg
Method还有q-value，不过好像我讲的时候，学生都没啥兴趣，或许有一天，他们写文章，reviewer要求给出FDR的时候，希望还能记起。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;4. The relationship between Pvalue, Effect size, and Sample size This
needs to be drilled into everyone - we&amp;rsquo;re far too trigger happy quoting
Pvalues, when we should often be quoting Pvalues and Effect size. Once a
Pvalue is significant, it&amp;rsquo;s higher significance is sort of meaningless
(or rather it compounds Effect size things with Sample size things, the
latter often being about relative frequency). So - if something is
significantly correlated/different, then you want to know about how much
of an effect this observation has. This is not just about GWAS like
statistics - in genomic biology we&amp;rsquo;re all too happy about quoting some
small Pvalue not realising that with a million or so points often, even
very small deviations will be significant. Quote your r2, Rhos or
proportion of variance explained&amp;hellip;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从没接触过GWAS，不知道是怎么算的，从文中的描述看，这里讲的是power
analysis，这个对实验设计有用，可以估计sample size。当然如果sample
size已确定，那么设定pvalue和power，可以计算effect
size，就是说，实验可以detect出多大的effect。或者知道sample size,effect
size,
pvalue，可以计算power，就是说effect存在的话，有多大的概率可以detect出来。这和pvalue不一样，pvalue算的是没有effect（$H_0$）的概率。&lt;/p&gt;

&lt;p&gt;power analysis就是四个变量，颠来倒去，知道三个，算第四个。&lt;/p&gt;

&lt;p&gt;差异越大，越容易检测出来，样本间的variance越大，就需要更大的样本量，来排除样本间差异所带来的干扰。差不多就是这样一些东西。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;5. Linear models and PCA. There is a tendency often to jump to quite
complex models - networks, or biologically inspired combinations, when
our first instinct should be to crack out the well established lm()
(linear model) for prediction and princomp() (PCA) for dimensionality
reduction. These are old school techniques - and often if you want to
talk about statistical fits one needs to make gaussian assumptions about
distributions - but most of the things we do could be either done well
in a linear model, and most of the correlation we look at could have
been found with a PCA biplot. The fact that these are 1970s bits of
statistics doesn&amp;rsquo;t mean they don&amp;rsquo;t work well.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;PCA就是把高维空间映射到低维空间，在保留尽可能多信息的情况下进行降维处理，
下面这段解释了linear model和PCA之间的不同之处：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;One may also see PCA as an analogue of the least squares method to
find a line that goes as “near” the points as possible – to simplify,
let us assume there are just two dimensions. But while the least
squares method is asymetric (the two variables play different roles:
they are not interchangeable, we try to predict one from the others,
we measure the distance parallel to one coordinate axis), the PCA is
symetric (the distance is measured orthogonally to the line we are
looking for).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;John Mark在评论里写道，进阶还需要学什么，一并记录下来。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The next level - number 6 - would be to get beyond P values, and
instead compute probability distributions of the quantities of
interest. This leads naturally to number 7, which is to delve into the
generative models that are currently solved by MCMC methods. This is
basically the Bayesian approach. Just as an aside &amp;ldquo;non parametrics&amp;rdquo; in
some new work is also used to mean models where the number of
parameters varies, as a consequence of the method.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Bootstrap Method</title>
      <link>http://guangchuangyu.github.io/2009/07/bootstrap-method/</link>
      <pubDate>Fri, 31 Jul 2009 09:14:42 +0800</pubDate>
      
      <guid>http://guangchuangyu.github.io/2009/07/bootstrap-method/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Bootstrapping_(statistics)&#34;&gt;bootstrap&lt;/a&gt;是对观测数据集进行有放回（replacement）的随机抽样，以评估总体的各项统计指标。可以用于假设检验、参数估计。好处是并不要求大样本，也不要求正态数据，并且对于不同的统计指标使用的是同样的计算方法。结果也更为可靠，坏处是计算量大。&lt;/p&gt;

&lt;p&gt;统计推断(statistical
inference)是基于样本统计值的抽样分布来计算的，抽样分布需要从总体中许多的样本来计算，在只有一个样本的情况下，bootstrap对这一随机样本进行有放回的重复抽样，每一个重抽样本与原始随机样本一样大，每次计算相应的抽样的统计值，重复了N次之后，就可以计算统计值的bootstrap分布。&lt;/p&gt;

&lt;p&gt;下面做一个小小的试验：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a &amp;lt;- c(seq(1:10), rnorm(50))  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;#创建一个样本，60个数据，非正态分布的，如下图&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guangchuangyu.github.io/blog_images/2009/07/sample_a.png&#34; alt=&#34;&#34; title=&#34;sample_a&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;b &amp;lt;- rep(0, 1000)
for (i in 1:1000) {
    b[i] &amp;lt;- mean(sample(a, replace=TRUE))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对它进行1000次有放回重复抽样，计算均值，均值分布的柱状图和qq图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://guangchuangyu.github.io/blog_images/2009/07/hist_qq.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;分布是正态的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; mean(a)    #样本均值
[1] 0.947186
&amp;gt; mean(b)   #重抽样1000个样本均值的均值
[1] 0.9358049
&amp;gt; sd(b)
[1] 0.3245479
&amp;gt; sd(a)/sqrt(60)
[1] 0.3318863
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由bootstrap方法得到的标准误是0.325，而由原先随机样本所估算的是0.332，两者是相当接近的。由此可见呢，bootstrap方法与理论上的从总体中抽样所得到的样本分布是一致的，这样子就可以基于一个样本来计算样本的分布，而不是要从总体中抽N个样本。&lt;/p&gt;

&lt;p&gt;还用上面的例子，使用样本均值$ \bar{x}$去估算总体的均值$
\mu$，虽然我们的样本不是正态的，但如果bootstrap
distribution是正态的话，就可以使用类似于单样本t置信区间（要求样本是正态的）的公式:$
\bar{x} \pm t \cdot SE = \bar{x} \pm t \cdot
\frac{s}{\sqrt{n}}$ 相应的bootstrap t置信区间是：$ statistic
\pm t \cdot SE_{boot}$ 上面例子，总体均值$ \mu$
95%的置信度，就会落在：$ mean(a) \pm 2.009 \cdot sd(b)$
可以使用它，计算所有统计指标的置信区间。&lt;/p&gt;

&lt;p&gt;用于层次聚类分析的一个例子，使用Pvclust这个包，用于层次聚类，并通过multiscale
bootstrap
resampling给出相应的&lt;em&gt;p&lt;/em&gt;-value用于评估聚类结果的不确定性。提供了两种&lt;em&gt;p&lt;/em&gt;-values，AU（approximately
unbiased p-value, 通过multiscale bootstrap
resampling计算）和BP（bootstrap probability, 通过normal bootstrap
resampling计算），AU比BP较为unbiased。
Pvclust使用*hclust*（来自于stats包）进行层次聚类，并自动计算所有的子类的&lt;em&gt;p&lt;/em&gt;-value。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(pvclust)  # loads package
a &amp;lt;- matrix(rnorm(1000), 100, 10, dimnames=list(paste(&amp;quot;g&amp;quot;, 1:100, sep=&amp;quot;&amp;quot;), paste(&amp;quot;t&amp;quot;,1:10, sep=&amp;quot;&amp;quot;)))  # creats sample data
at &amp;lt;- t(a)  # transposes of a
cl &amp;lt;- pvclust(at, nboot=1000)  #performs hierarchical cluster analysis with multiscale bootstrap with 1000 repetitions.
pvrect(cl, alpha=0.95)  #highlights with red rectangles all clusters in the dendrogram which have an AU value above 95%, AU p-value &amp;gt; 0.95, the hypothesis that &amp;quot;the cluster does not exist&amp;quot; is rejected with significance level 0.05;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://guangchuangyu.github.io/blog_images/2009/07/pvclust_test.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#pick significant clusters.
clsig &amp;lt;- pvpick(cl, alpha=0.95, pv=&amp;quot;au&amp;quot;, type=&amp;quot;geq&amp;quot;, max.only=TRUE)  
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;reference:5129f3fc49180ee68d0efdb6ded483c8&#34;&gt;Reference&lt;/h3&gt;

&lt;p&gt;1.  Moore, D.S., McCabe, G.P. &amp;amp; Craig, B. Introduction to the Practice
of Statistics w/CD-ROM.  (W. H. Freeman: 2007).&lt;/p&gt;

&lt;p&gt;2.  Suzuki, R. (2006). Pvclust: an R package for assessing the uncertainty in hierarchical clustering, Bioinformatics, 22(12), 1540-1542
DOI:
&lt;a href=&#34;http://dx.doi.org/10.1093/bioinformatics/btl117&#34;&gt;10.1093/bioinformatics/btl117&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
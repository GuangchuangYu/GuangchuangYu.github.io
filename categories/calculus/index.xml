<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Calculus | Guangchuang YU</title>
    <link>https://guangchuangyu.github.io/categories/calculus/</link>
      <atom:link href="https://guangchuangyu.github.io/categories/calculus/index.xml" rel="self" type="application/rss+xml" />
    <description>Calculus</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019 Guangchuang YU</copyright><lastBuildDate>Sat, 01 Jan 2011 08:52:48 +0800</lastBuildDate>
    <image>
      <url>https://guangchuangyu.github.io/img/icon-192.png</url>
      <title>Calculus</title>
      <link>https://guangchuangyu.github.io/categories/calculus/</link>
    </image>
    
    <item>
      <title>Single variable optimization</title>
      <link>https://guangchuangyu.github.io/post/r/spurs/2011_single-variable-optimization/</link>
      <pubDate>Sat, 01 Jan 2011 08:52:48 +0800</pubDate>
      <guid>https://guangchuangyu.github.io/post/r/spurs/2011_single-variable-optimization/</guid>
      <description>&lt;p&gt;Optimization means to seek minima or maxima of a funtion within a given defined domain.&lt;/p&gt;

&lt;p&gt;If a function reach its maxima or minima, the derivative at that point is approaching to 0. If we apply &lt;a href=&#34;https://mp.weixin.qq.com/s/qhT_sooSINjjvWpwq2d1VA&#34; target=&#34;_blank&#34;&gt;Newton-Raphson method&lt;/a&gt; for root finding to &lt;code&gt;f&#39;&lt;/code&gt;, we can get the optimized &lt;code&gt;f&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;f2df &amp;lt;- function(fun) {
    fun.list = as.list(fun)
    var &amp;lt;- names(fun.list[1])
    fun.exp = fun.list[[2]] 
    diff.fun = D(fun.exp, var) 
    df = list(x=0, diff.fun) 
    df = as.function(df) 
    return(df)
}

newton &amp;lt;- function(fun, x0, tol=1e-7, niter=100) { 
    df = f2df(fun) 
    for (i in 1:niter) { 
        x = x0 - fun(x0)/df(x0) 
        if (abs(fun(x)) &amp;lt; tol) 
            return(x) 
        x0 = x      
    } 
    stop(&amp;quot;exceeded allowed number of iterations&amp;quot;) 
}
    
newton_optimize &amp;lt;- function(fun, x0, tol=1e-7, niter=100) {
    df &amp;lt;- f2df(fun)
    x = newton(df, x0, tol, niter)
    ddf &amp;lt;- f2df(df)
    if (ddf(x) &amp;gt; 0) {
        cat (&amp;quot;minima:\t&amp;quot;, x, &amp;quot;\n&amp;quot;)
    } else {
        cat (&amp;quot;maxima:\t&amp;quot;, x, &amp;quot;\n&amp;quot;)
    }
    return(x)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;a href=&#34;http://en.wikipedia.org/wiki/Golden_section_search&#34; target=&#34;_blank&#34;&gt;golden-section method&lt;/a&gt; does not need &lt;code&gt;f&#39;&lt;/code&gt;. And it is similar to the root-bracketing technique for root finding.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gSection &amp;lt;- function(f, x1, x2, x3, tol=1e-7) {
    r &amp;lt;- 2 - (1+sqrt(5))/2 
    x4 &amp;lt;- x2 + (x3-x2)*r
    if ( abs(x3-x1) &amp;lt; tol ){
        return(x2)
    }
    if (f(x4) &amp;lt; f(x2)) {
        gSection(f, x2, x4, x3, tol)
    } else {
        gSection(f, x4, x2, x1, tol)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; f &amp;lt;- function(x) (x-1/3)^2
&amp;gt; newton_optimize(f, 0, tol=1e-7)
minima:  0.3333333 
[1] 0.3333333
&amp;gt; gSection(f, 0,0.5,1)
[1] 0.3333333
&amp;gt; optimize(f, c(0,1), tol=1e-7)
$minimum
[1] 0.3333333

$objective
[1] 0
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>one-dimensional integrals</title>
      <link>https://guangchuangyu.github.io/post/r/spurs/one-dimensional-integrals/</link>
      <pubDate>Sat, 25 Dec 2010 10:40:22 +0800</pubDate>
      <guid>https://guangchuangyu.github.io/post/r/spurs/one-dimensional-integrals/</guid>
      <description>

&lt;p&gt;The foundamental idea of numerical integration is to estimate the area of the region in the &lt;em&gt;xy&lt;/em&gt;-plane bounded by the graph of function &lt;em&gt;f(x)&lt;/em&gt;. The integral was esimated by dividing &lt;em&gt;x&lt;/em&gt; into small intervals, then adds all the small approximations to give a total approximation.&lt;/p&gt;

&lt;h2 id=&#34;trapezoidal-rule&#34;&gt;Trapezoidal rule&lt;/h2&gt;

&lt;p&gt;Numerical integration can be done by trapezoidal rule, simpson&amp;rsquo;s rule and quadrature rules. R has a built-in function, &lt;code&gt;integrate&lt;/code&gt;, which performs adaptive quadrature.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Trapezoidal_rule&#34; target=&#34;_blank&#34;&gt;Trapezoidal rule&lt;/a&gt; works by approximating the region under the graph &lt;em&gt;f(x)&lt;/em&gt; as a trapezoid and calculating its area.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;trapezoid &amp;lt;- function(fun, a, b, n=100) {
    # numerical integral of fun from a to b
    # using the trapezoid rule with n subdivisions
    # assume a &amp;lt; b and n is a positive integer
    h &amp;lt;- (b-a)/n
    x &amp;lt;- seq(a, b, by=h)
    y &amp;lt;- fun(x)
    s &amp;lt;- h * (y[1]/2 + sum(y[2:n]) + y[n+1]/2)
    return(s)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;simpson-s-rule&#34;&gt;Simpson&amp;rsquo;s rule&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Simpson%27s_rule&#34; target=&#34;_blank&#34;&gt;Simpson&amp;rsquo;s rule&lt;/a&gt; subdivides the interval &lt;code&gt;[a,b]&lt;/code&gt; into &lt;em&gt;n&lt;/em&gt; subintervals, where &lt;em&gt;n&lt;/em&gt; is even, then on each consecutive pairs of subintervals, it approximates the behaviour of &lt;em&gt;f(x)&lt;/em&gt; by a parabola (polynomial of degree 2) rather than by the straight lines used in the trapezoidal rule.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;simpson &amp;lt;- function(fun, a, b, n=100) {
    # numerical integral using Simpson&#39;s rule
    # assume a &amp;lt; b and n is an even positive integer
    h &amp;lt;- (b-a)/n
    x &amp;lt;- seq(a, b, by=h)
    if (n == 2) {
        s &amp;lt;- fun(x[1]) + 4*fun(x[2]) +fun(x[3])
    } else {
        s &amp;lt;- fun(x[1]) + fun(x[n+1]) +
        	2*sum(fun(x[seq(2,n,by=2)])) + 
            4 *sum(fun(x[seq(3,n-1, by=2)]))
    }
    s &amp;lt;- s*h/3
    return(s)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To calculate an integrate over infinite interval, one way is to
transform it into an integral over a finite interval as introduce in &lt;a href=&#34;http://en.wikipedia.org/wiki/Numerical_integration&#34; target=&#34;_blank&#34;&gt;wiki&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;simpson_v2 &amp;lt;- function(fun, a, b, n=100) {
    # numerical integral using Simpson&#39;s rule
    # assume a &amp;lt; b and n is an even positive integer
    if (a == -Inf &amp;amp; b == Inf) {
        f &amp;lt;- function(t) (fun((1-t)/t) + fun((t-1)/t))/t^2
        s &amp;lt;- simpson_v2(f, 0, 1, n)
    } else if (a == -Inf &amp;amp; b != Inf) {
        f &amp;lt;- function(t) fun(b-(1-t)/t)/t^2
        s &amp;lt;- simpson_v2(f, 0, 1, n)
    } else if (a != -Inf &amp;amp; b == Inf) {
        f &amp;lt;- function(t) fun(a+(1-t)/t)/t^2
        s &amp;lt;- simpson_v2(f, 0, 1, n)
    } else {
        h &amp;lt;- (b-a)/n
        x &amp;lt;- seq(a, b, by=h)
        y &amp;lt;- fun(x)
        y[is.nan(y)]=0
        s &amp;lt;- y[1] + y[n+1] + 
        	2*sum(y[seq(2,n,by=2)]) + 
            4 *sum(y[seq(3,n-1, by=2)])
        s &amp;lt;- s*h/3
    }
    return(s)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; phi &amp;lt;- function(x) exp(-x^2/2)/sqrt(2*pi)  ##normal distribution
&amp;gt; simpson_v2(phi, 0, Inf, n=100)
[1] 0.4986569
&amp;gt; simpson_v2(phi, -Inf,3, n=100)
[1] 0.998635
&amp;gt; pnorm(3)
[1] 0.9986501
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Simpson&amp;rsquo;s rule is more accuracy than trapezoidal rule. To compare the accuracy between simpson&amp;rsquo;s rule and trapezoidal rule, I estimated $ \int_{0.01}^{1} \frac{1}{x} dx = -\log(0.01)$ for a sequence of increasing values of &lt;em&gt;n&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;f &amp;lt;- function(x) 1/x
#integrate(f, 0.01, 1) == -log(0.01)
S.trapezoid &amp;lt;- function(n) trapezoid(f, 0.01, 1, n)
S.simpson &amp;lt;- function(n) simpson(f, 0.01, 1, n)

n &amp;lt;- seq(10, 1000, by = 10)
St &amp;lt;- sapply(n, S.trapezoid)
Ss &amp;lt;- sapply(n, S.simpson)

opar &amp;lt;- par(mfrow = c(2, 2))
plot(n,St + log(0.01), type=&amp;quot;l&amp;quot;, xlab=&amp;quot;n&amp;quot;, ylab=&amp;quot;error&amp;quot;, main=&amp;quot;Trapezoidal rule&amp;quot;)
plot(n,Ss + log(0.01), type=&amp;quot;l&amp;quot;, xlab=&amp;quot;n&amp;quot;, ylab=&amp;quot;error&amp;quot;,main=&amp;quot;Simpson&#39;s rule&amp;quot;)
plot(log(n), log(St+log(0.01)),type=&amp;quot;l&amp;quot;, xlab=&amp;quot;log(n)&amp;quot;, ylab=&amp;quot;log(error)&amp;quot;,main=&amp;quot;Trapezoidal rule&amp;quot;)
plot(log(n), log(Ss+log(0.01)),type=&amp;quot;l&amp;quot;, xlab=&amp;quot;log(n)&amp;quot;, ylab=&amp;quot;log(error)&amp;quot;,main=&amp;quot;Simpson&#39;s rule&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://guangchuangyu.github.io/blog_images/2010/12/test-accuarcy.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The plot showed that log(error) against log(n) appears to have a slope of -1.90 and -3.28 for trapezoidal rule and simpson&amp;rsquo;s rule respectively.&lt;/p&gt;

&lt;p&gt;Another way to compare their accuracy is to calculate how large of the partition size &lt;em&gt;n&lt;/em&gt; for reaching a specific tolerance.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;yIntegrate &amp;lt;- function(fun, a, b, tol=1e-8, method= &amp;quot;simpson&amp;quot;, verbose=TRUE) {
    # numerical integral of fun from a to b, assume a &amp;lt; b 
    # with tolerance tol
    n &amp;lt;- 4
    h &amp;lt;- (b-a)/4
    x &amp;lt;- seq(a, b, by=h)
    y &amp;lt;- fun(x)
    yIntegrate_internal &amp;lt;- function(y, h, n, method) {
        if (method == &amp;quot;simpson&amp;quot;) {
            s &amp;lt;- y[1] + y[n+1] + 4*sum(y[seq(2,n,by=2)]) + 
            	2 *sum(y[seq(3,n-1, by=2)])
            s &amp;lt;- s*h/3
        } else if (method == &amp;quot;trapezoidal&amp;quot;) {
            s &amp;lt;- h * (y[1]/2 + sum(y[2:n]) + y[n+1]/2)
        } else {
        }       
        return(s)
    }
    
    s &amp;lt;- yIntegrate_internal(y, h, n, method)
    s.diff &amp;lt;- tol + 1 # ensures to loop at once.
    while (s.diff &amp;gt; tol ) {
        s.old &amp;lt;- s
        n &amp;lt;- 2*n
        h &amp;lt;- h/2
        y[seq(1, n+1, by=2)] &amp;lt;- y ##reuse old fun values
        y[seq(2,n, by=2)] &amp;lt;- sapply(seq(a+h, b-h, by=2*h), fun)
        s &amp;lt;- yIntegrate_internal(y, h, n, method)
        s.diff &amp;lt;- abs(s-s.old)
    }
    if (verbose) {
        cat(&amp;quot;partition size&amp;quot;, n, &amp;quot;\n&amp;quot;)
    }
    return(s)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; fun &amp;lt;- function(x) exp(x) - x^2
&amp;gt; yIntegrate(fun, 3,5,tol=1e-8, method=&amp;quot;simpson&amp;quot;)
partition size 512 
[1] 95.66096
&amp;gt; yIntegrate(fun, 3,5,tol=1e-8, method=&amp;quot;trapezoidal&amp;quot;)
partition size 131072 
[1] 95.66096
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As show above, simpson&amp;rsquo;s rule converge much faster than trapezoidal rule.&lt;/p&gt;

&lt;h2 id=&#34;quadrature-rule&#34;&gt;Quadrature rule&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Adaptive_quadrature&#34; target=&#34;_blank&#34;&gt;Quadrature rule&lt;/a&gt; is
more efficient than traditional algorithms. In adaptive quadrature, the subinterval width &lt;code&gt;h&lt;/code&gt; is not constant over the interval &lt;code&gt;[a,b]&lt;/code&gt;, but instead adapts to the function. Here, I presented the adaptive simpson&amp;rsquo;s method.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;quadrature &amp;lt;- function(fun, a, b, tol=1e-8) {
    # numerical integration using adaptive quadrature
    
    quadrature_internal &amp;lt;- function(S.old, fun, a, m, b, tol, level) {
        level.max &amp;lt;- 100
        if (level &amp;gt; level.max) {
            cat (&amp;quot;recursion limit reached: singularity likely\n&amp;quot;)
            return (NULL)
        }
        S.left &amp;lt;- simpson(fun, a, m, n=2) 
        S.right &amp;lt;- simpson(fun, m, b, n=2)
        S.new &amp;lt;- S.left + S.right
        if (abs(S.new-S.old) &amp;gt; tol) {
            S.left &amp;lt;- quadrature_internal(S.left, fun, a, (a+m)/2, m, tol/2, level+1)
            S.right &amp;lt;- quadrature_internal(S.right, fun, m, (m+b)/2, b, tol/2, level+1)
            S.new &amp;lt;- S.left + S.right
        }
        return(S.new)
    }
    
    level = 1
    S.old &amp;lt;- (b-a) * (fun(a) + fun(b))/2
    S.new &amp;lt;- quadrature_internal(S.old, fun, a, (a+b)/2, b, tol, level+1)
    return(S.new)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Quadrature rule is effective when &lt;em&gt;f(x)&lt;/em&gt; is steep.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; fun &amp;lt;- function(x) return(1.5 * sqrt(x))
&amp;gt; system.time(yIntegrate(fun, 0,1, tol=1e-9, method=&amp;quot;simpson&amp;quot;))
partition size 524288 
   user  system elapsed 
   1.58    0.00    1.58 
&amp;gt; system.time(quadrature(fun, 0,1, tol=1e-9))
   user  system elapsed 
   0.25    0.00    0.25 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Robinson A., O. Jones, and R. Maillardet. 2009.
&lt;a href=&#34;http://book.douban.com/subject/4324593/&#34; target=&#34;_blank&#34;&gt;Introduction to Scientific Programming and Simulation Using
R&lt;/a&gt;. Chapman and Hall.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Numerical_integration&#34; target=&#34;_blank&#34;&gt;http://en.wikipedia.org/wiki/Numerical_integration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Trapezoidal_rule&#34; target=&#34;_blank&#34;&gt;http://en.wikipedia.org/wiki/Trapezoidal_rule&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Simpson%27s_rule&#34; target=&#34;_blank&#34;&gt;http://en.wikipedia.org/wiki/Simpson%27s_rule&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Adaptive_quadrature&#34; target=&#34;_blank&#34;&gt;http://en.wikipedia.org/wiki/Adaptive_quadrature&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Root finding</title>
      <link>https://guangchuangyu.github.io/post/r/spurs/2010_root-finding/</link>
      <pubDate>Sat, 04 Dec 2010 10:40:22 +0800</pubDate>
      <guid>https://guangchuangyu.github.io/post/r/spurs/2010_root-finding/</guid>
      <description>&lt;p&gt;Numerical root finding methods use iteration, producing a sequence of numbers that hopefully converge towards a limits which is a root. This post only focuses four basic algorithms on root finding, and covers bisection method, fixed point method, Newton-Raphson method, and secant method.&lt;/p&gt;

&lt;p&gt;The simplest root finding algorithms is the &lt;a href=&#34;https://guangchuangyu.github.io/cn/2008/11/bisect-to-solve-equation/&#34; target=&#34;_blank&#34;&gt;bisection method&lt;/a&gt;. It works when &lt;code&gt;f&lt;/code&gt; is a continuous function and it requires previous knowledge of two initial gueeses, &lt;code&gt;u&lt;/code&gt; and &lt;code&gt;v&lt;/code&gt;, such that &lt;code&gt;f(u)&lt;/code&gt; and &lt;code&gt;f(v)&lt;/code&gt; have opposite signs. This method is reliable, but converges slowly. For detail, see &lt;a href=&#34;https://guangchuangyu.github.io/cn/2008/11/bisect-to-solve-equation/&#34; target=&#34;_blank&#34;&gt;https://guangchuangyu.github.io/cn/2008/11/bisect-to-solve-equation/&lt;/a&gt; .&lt;/p&gt;

&lt;p&gt;Root finding can be reduced to the problem of finding fixed points of the function &lt;code&gt;g(x) = c*f(x) +x&lt;/code&gt;, where &lt;code&gt;c&lt;/code&gt; is a non-zero constant. It is clearly that &lt;code&gt;f(a) = 0&lt;/code&gt; if and only if &lt;code&gt;g(a) = a&lt;/code&gt;. This is the so called fixed point algorithm.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;fixedpoint &amp;lt;- function(fun, x0, tol=1e-07, niter=500){
    ## fixed-point algorithm to find x such that fun(x) == x
    ## assume that fun is a function of a single variable
    ## x0 is the initial guess at the fixed point
 
    xold &amp;lt;- x0
    xnew &amp;lt;- fun(xold)
    for (i in 1:niter) {
        xold &amp;lt;- xnew
        xnew &amp;lt;- fun(xold)
        if ( abs((xnew-xold)) &amp;lt; tol )
            return(xnew)
        }
    stop(&amp;quot;exceeded allowed number of iterations&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; f &amp;lt;- function(x) log(x) - exp(-x)
&amp;gt; gfun &amp;lt;- function(x) x - log(x) + exp(-x)
&amp;gt; fixedpoint(gfun, 2)
[1] 1.309800
&amp;gt; x=fixedpoint(gfun, 2)
&amp;gt; f(x)
[1] 3.260597e-09
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The fixed point algorithm is not reliable, since it cannot guaranteed to
converge. Another disavantage of this method is that the speed is relatively slow.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://guangchuangyu.github.io/cn/2007/06/newton-raphson-method/&#34; target=&#34;_blank&#34;&gt;Newtom-Raphson method&lt;/a&gt; converge more quickly than bisection method and fixed point method. It assumes the function &lt;code&gt;f&lt;/code&gt; to have a continuous derivative. For detail, see &lt;a href=&#34;http://guangchuangyu.github.io/cn/2007/06/newton-raphson-method/&#34; target=&#34;_blank&#34;&gt;http://guangchuangyu.github.io/cn/2007/06/newton-raphson-method/&lt;/a&gt; .&lt;/p&gt;

&lt;p&gt;The secant method does not require the computation of a derivative, it only requires that the function &lt;code&gt;f&lt;/code&gt; is continuous. The secant method is based on a linear approximation to the function &lt;code&gt;f&lt;/code&gt;. The convergence properties of the secant method are similar to those of the Newton-Raphson method.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;secant &amp;lt;- function(fun, x0, x1, tol=1e-07, niter=500){
    for ( i in 1:niter ) {
        x2 &amp;lt;- x1-fun(x1)*(x1-x0)/(fun(x1)-fun(x0))
        if (abs(fun(x2)) &amp;lt; tol)
            return(x2)
        x0 &amp;lt;- x1
        x1 &amp;lt;- x2
    }
    stop(&amp;quot;exceeded allowed number of iteractions&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; f &amp;lt;- function(x) log(x) - exp(-x)
&amp;gt; secant(f, x0=1, x1=2)
[1] 1.309800
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
  </channel>
</rss>
